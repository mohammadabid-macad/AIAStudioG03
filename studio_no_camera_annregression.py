# -*- coding: utf-8 -*-
"""Studio_no camera_ANNregression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W1vOi8Q6Q2Vqnkuo7KZwiYSFqD8T87A8

# Classification using Artificial Neural Networks

#Numerical regression

in this example we will use Keras to predict wages based on various professional and demographic factors.
"""

# Import standard Libraries
import pandas as pd
import seaborn as sns
import altair as alt
import tensorflow as tf
import matplotlib.pyplot as plt



sns.set(rc={'figure.figsize':(10,10)})
print("imports ok")

"""##Load Data"""

#connect google drive to this colab
from google.colab import drive
drive.mount('/content/gdrive')

import pickle

# Load the .pkl file
with open('/content/buildings_with_materials_v3.pkl', 'rb') as f:
    data = pickle.load(f)

# Now you can inspect or use the loaded data
print(data)

print(data.info())

# prompt: in the buildings_with_materials create a column called 'Building_Height' which multiplies the building levels to 3 (our floor to floor height)

data['Building_Heights'] = data['building:levels'] * 3

data.describe()

"""we can see that some predictors are binary, while others are not

##Category encoding
"""

# Get unique values in the 'category' column
unique_values = data['Category'].unique()

# Print the unique values
print(unique_values)

from sklearn.preprocessing import OneHotEncoder

# Replace 'nan' string with actual NaN value if necessary
data['Category'].replace('nan', pd.NA, inplace=True)

# One-hot encode the 'Category' feature
encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
category_encoded = encoder.fit_transform(data[['Category']])

# Create a DataFrame with the encoded category columns
category_encoded_df = pd.DataFrame(category_encoded, columns=encoder.get_feature_names_out(['Category']))

# Drop the original 'Category' column and concatenate the encoded columns
data = data.drop(columns=['Category']).reset_index(drop=True)
data = pd.concat([data, category_encoded_df], axis=1)

print(data)

"""##Prepare Data

**NORMALIZE INPUTS**
"""

# Define the columns you want to keep
columns_to_keep = ["Building_FootprintArea", "Category_commercial", "Category_education", "Category_hospitality", "Category_industrial", "Category_office",  "Category_institution", "Category_residential", "Category_retail", "Category_nan", "latitude", "longitude", "brick", "ceramic", "glass", "metal", "paint", "tile", "wood" ]

# Keep only the columns you want
features_data = data[columns_to_keep]

# Now you can inspect or use the data with only the desired columns
print(features_data)

# Convert material columns to numeric if possible
material_columns = ["brick", "ceramic", "glass", "metal", "paint", "tile", "wood"]
for col in material_columns:
    features_data[col] = pd.to_numeric(features_data[col], errors='coerce')

# Fill or drop NaN values in material columns as appropriate (here we fill with 0)
features_data[material_columns] = features_data[material_columns].fillna(0)

print(features_data)

#same code as last time

#declare features
X = features_data

# Load and instantiate a StandardSclaer
from sklearn.preprocessing import StandardScaler
scalerX = StandardScaler()

# Apply the scaler to our X-features
X_scaled = scalerX.fit_transform(X)

print(X_scaled.shape)

# Declare regression target
y_columns = ["Building_Heights"]
y = data.loc[:, y_columns].to_numpy()


from sklearn.preprocessing import MinMaxScaler
scalerY = MinMaxScaler()

# Apply the scaler to our y-features
y_scaled = scalerY.fit_transform(y)

print(y_scaled.shape)

"""**SPLIT INTO TRAIN AND TEST**"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size = 0.2, random_state = 21)

#visualize our data
#we can see that scikitlearn doesnt care if it is a dataframe or a numpy array, because they all function on the same way
print("TRAIN", "input", X_train.shape, "output", y_train.shape)
print("TEST", "input", X_test.shape, "output", y_test.shape)

#SAVE SCALER FOR LATER USE

import pickle
with open('Sscaler.pkl', 'wb') as f:
    pickle.dump(scalerX, f)

"""# removing outliers"""

import numpy as np

Q1 = np.percentile(X, 25, axis=0)
Q3 = np.percentile(X, 75, axis=0)
IQR = Q3 - Q1
lower_bound = Q1 - 9 * IQR
upper_bound = Q3 + 9 * IQR

mask = np.all((X >= lower_bound) & (X <= upper_bound), axis=1)

X_no_outliers = X[mask]
y_no_outliers = y[mask]

print(X_no_outliers.shape)
print(y_no_outliers.shape)

# Scaling the cleaned data
X_scaled = scalerX.fit_transform(X_no_outliers)
y_scaled = scalerY.fit_transform(y_no_outliers)

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=21)

#visualize our data
#we can see that scikitlearn doesnt care if it is a dataframe or a numpy array, because they all function on the same way
print("TRAIN", "input", X_train.shape, "output", y_train.shape)
print("TEST", "input", X_test.shape, "output", y_test.shape)

#SAVE SCALER FOR LATER USE

import pickle
with open('Sscaler_nooutlier.pkl', 'wb') as f:
    pickle.dump(scalerX, f)

"""##Build model

From the cheatSheet
Regression between 0 and 1>>
      activation = relu for hidden layers / sigmoid for final layer
      loss = mean squared error
      optimizer = adam
      input from data, is 8 columns
      output is 1 value prediction
"""

from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from keras.regularizers import l2

# Build the model
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=X_train.shape[1], kernel_regularizer=l2(0.01)))
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])

# Callbacks for early stopping and learning rate reduction
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)

# Train the model
history = model.fit(X_train, y_train, epochs=500, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])

model.save('studio_trained_controlled.h5')

model.save('studio_trained_controlled.keras')

model.summary()

# summarize history for accuracy
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('loss function')
plt.ylabel('mse')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""##Evaluate model on test data


"""

# Evaluate your model accuracy on the test data
loss_test = model.evaluate(X_test,y_test)

# Print accuracy
print('mse_test:', loss_test)

from sklearn.metrics import mean_absolute_error

# Assuming you have trained your model and made predictions
# Here, y_pred represents the predicted values
y_pred = model.predict(X_test)

# Now you can calculate the mean absolute error
mae = mean_absolute_error(y_test, y_pred)
print(mae)

"""##Plot error

"""

def plot_comparison(x_val, pred, truth, xlab, ylab):
  fig, ax1 = plt.subplots()
  ax1.plot(x_val, truth, color = "red", label = "truth",linestyle='None', marker = "o", markersize = 5)
  ax1.plot(x_val, pred, color = "blue", label = "pred",linestyle='None', marker = "o", markersize = 4, alpha = 0.5)

  ax1.set_xlabel(xlab)
  ax1.set_ylabel(ylab)
  ax1.legend()
  fig.set_figheight(10)
  fig.set_figwidth(20)
  plt.title('Prediction Comparison')
  plt.show()

y_pred = scalerY.inverse_transform(model.predict(X_test))
y_truth = scalerY.inverse_transform(y_test)

plt.scatter(y_truth,y_pred)
plt.ylim(5,20)

error = y_pred - y_truth
plt.hist(error, bins=25)
plt.xlabel('Prediction Error')
_ = plt.ylabel('Count')

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming the model and data (X_test, y_test, y) are already defined

# Make predictions on the scaled test data
y_pred_scaled = model.predict(X_test)

# Initialize MinMaxScaler for inverse transformation
scaler_y = MinMaxScaler()
scaler_y.fit(y)

# Inverse transform the predictions and true values for all columns
y_pred = scaler_y.inverse_transform(y_pred_scaled)
y_true = scaler_y.inverse_transform(y_test)

# Select 10 random indices to compare
random_indices = np.random.randint(0, len(X_test), 10)

# Prepare data for CSV
data = {
    "Random_Index": [],
    "Building_Heights": [],
    "True_Building_Heights": []
}

for idx in random_indices:
    data["Random_Index"].append(idx)
    for i, column in enumerate(["Building_Heights"]):
        data[column].append(y_pred[idx, i])
        data[f"True_{column}"].append(y_true[idx, i])

# Create DataFrame
df = pd.DataFrame(data)

# Save to CSV
df.to_csv("predictions_comparison1.csv", index=False)

print("CSV file 'predictions_comparison1.csv' created successfully.")

# Reverse scaling the predictions
y_pred = scalerY.inverse_transform(y_pred_scaled)

# Reverse scaling the test targets for comparison
y_test_original = scalerY.inverse_transform(y_test)

# Save the model as a .keras file
model.save('inverse_scaled.keras')

print("Model has been saved successfully as model.keras.")